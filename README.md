- **Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach**
    >We considered the Federated Learning (FL) problem in the heterogeneous case, and studied a
personalized variant of the classic FL formulation in which our goal is to find a proper initialization
model for the users that can be quickly adapted to the local data of each user after the training phase.
We highlighted the connections of this formulation with Model-Agnostic Meta-Learning (MAML),
and showed how the decentralized implementation of MAML, which we called Per-FedAvg, can be
used to solve the proposed personalized FL problem. We also characterized the overall complexity of
Per-FedAvg for achieving first-order optimality in nonconvex settings. Finally, we provided a set of
numerical experiments to illustrate the performance of two different first-order approximations of
Per-FedAvg and their comparison with the FedAvg method, and showed that the solution obtained by
Per-FedAvg leads to a more personalized solution compared to the solution of FedAvg

- **Tackling the Objective Inconsistency Problem
in Heterogeneous Federated Optimization**
   > This paper provides a general framework to analyze the convergence of heterogeneous federated optimization algorithms. It subsumes previously
proposed methods such as FedAvg and FedProx, and provides the first principled
understanding of the solution bias and the convergence slowdown due to objective
inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving
fast error convergence

- **Attack of the Tails: Yes, You Really Can Backdoor Federated Learning**
