{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkulium/notes/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRrWtX0PXVr"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IST-DASLab/sparsegpt/blob/master/demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMUp4UrWjp-8"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VdbD9blm6j_r"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhSblKg_jter"
      },
      "source": [
        "Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nCz469NhV3c",
        "outputId": "5af90157-5a0c-4c0c-b013-e18e737e0ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'sparsegpt' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IST-DASLab/sparsegpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbM_bJODjyBg"
      },
      "source": [
        "### Pruning example\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om0QSLnLj8JN"
      },
      "source": [
        "Below we will show an example of SparseGPT applied to OPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9NTGmD4iVK7",
        "outputId": "63888067-273f-4a84-857d-47914da42f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/sparsegpt\n"
          ]
        }
      ],
      "source": [
        "%cd sparsegpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbiDyjx9j61I"
      },
      "source": [
        "Crerate directory to store prune model(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pJ-jauI-iyvi"
      },
      "outputs": [],
      "source": [
        "!mkdir -p sparse_opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGLiExo5Ksc4"
      },
      "source": [
        "We will use `opt.py` script to prune the model.\n",
        "Select one of the following OPT versions to fit into colab (with `bitsandbytes` one should be able to use larger 6.7b and 13b models):\n",
        "* facebook/opt-125m\n",
        "* facebook/opt-350m\n",
        "* facebook/opt-1.3b\n",
        "\n",
        "To prune the model select dataset for calibration (`c4`, `ptb` or `wikitext`). The SparseGPT paper uses `c4` by default.\n",
        "\n",
        "One can prune model to uniform sparsity with SparseGPT either with unstructured pruning or semistructured `N:M` pattern.\n",
        "\n",
        "To apply unstructured pruning specify `--sparsity` - floating point number in `[0, 1]`.\n",
        "\n",
        "For semitstructured specify `--prunen` and `--prunem` arguments - integer numbers.\n",
        "\n",
        "To apply magnitude pruning instead of SparseGPT select `--gmp` option.\n",
        "\n",
        "To apply quantization on top of sparsity specify `--wbits`.\n",
        "\n",
        "In the example below we prune `acebook/opt-125m` to 0.5 unstructured sparsity via SparseGPT. Try different options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxucjXmCibnI",
        "outputId": "0e1c9bf3-271e-4aeb-c02e-db3494bba5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-25 20:25:15.324885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Starting ...\n",
            "Ready.\n",
            "0 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.54\n",
            "error 13941.587890625\n",
            "0 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 547.517578125\n",
            "0 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 14149.1669921875\n",
            "0 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 6.390275001525879\n",
            "0 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 2260.54931640625\n",
            "0 fc2\n",
            "Pruning ...\n",
            "time 0.71\n",
            "error 36.479331970214844\n",
            "1 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.60\n",
            "error 9682.548828125\n",
            "1 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.28\n",
            "error 641.5951538085938\n",
            "1 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.28\n",
            "error 4022.392578125\n",
            "1 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.24\n",
            "error 4.027432441711426\n",
            "1 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 6266.3798828125\n",
            "1 fc2\n",
            "Pruning ...\n",
            "time 0.69\n",
            "error 14.219343185424805\n",
            "2 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.48\n",
            "error 15789.4208984375\n",
            "2 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.20\n",
            "error 1678.421875\n",
            "2 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 14260.4677734375\n",
            "2 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 9.766499519348145\n",
            "2 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 5069.49609375\n",
            "2 fc2\n",
            "Pruning ...\n",
            "time 0.71\n",
            "error 10.688285827636719\n",
            "3 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.48\n",
            "error 14277.4296875\n",
            "3 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 2320.9306640625\n",
            "3 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 14309.693359375\n",
            "3 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 15.976364135742188\n",
            "3 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 3086.05517578125\n",
            "3 fc2\n",
            "Pruning ...\n",
            "time 0.98\n",
            "error 0.48200276494026184\n",
            "4 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 27314.748046875\n",
            "4 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 3171.47998046875\n",
            "4 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 29181.6953125\n",
            "4 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 24.271169662475586\n",
            "4 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 7956.150390625\n",
            "4 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 29.87113380432129\n",
            "5 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 29646.88671875\n",
            "5 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 2876.80419921875\n",
            "5 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 34879.80078125\n",
            "5 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 38.6750602722168\n",
            "5 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 8126.98583984375\n",
            "5 fc2\n",
            "Pruning ...\n",
            "time 0.69\n",
            "error 71.48601531982422\n",
            "6 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.57\n",
            "error 31993.59375\n",
            "6 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.24\n",
            "error 3910.056884765625\n",
            "6 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.23\n",
            "error 33820.984375\n",
            "6 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.26\n",
            "error 48.310401916503906\n",
            "6 fc1\n",
            "Pruning ...\n",
            "time 0.23\n",
            "error 8047.31103515625\n",
            "6 fc2\n",
            "Pruning ...\n",
            "time 1.04\n",
            "error 101.61123657226562\n",
            "7 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.50\n",
            "error 39261.01171875\n",
            "7 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 4540.8515625\n",
            "7 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 39027.109375\n",
            "7 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 82.00370788574219\n",
            "7 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 10329.146484375\n",
            "7 fc2\n",
            "Pruning ...\n",
            "time 0.72\n",
            "error 137.32748413085938\n",
            "8 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 40062.2421875\n",
            "8 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 6433.7001953125\n",
            "8 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.20\n",
            "error 44656.8046875\n",
            "8 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 160.28634643554688\n",
            "8 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 14348.458984375\n",
            "8 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 227.66287231445312\n",
            "9 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.58\n",
            "error 47157.65625\n",
            "9 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.30\n",
            "error 7610.232421875\n",
            "9 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.31\n",
            "error 51200.7421875\n",
            "9 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.29\n",
            "error 271.1785583496094\n",
            "9 fc1\n",
            "Pruning ...\n",
            "time 0.29\n",
            "error 19706.921875\n",
            "9 fc2\n",
            "Pruning ...\n",
            "time 0.87\n",
            "error 372.0923156738281\n",
            "10 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.51\n",
            "error 44735.5078125\n",
            "10 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 9526.2900390625\n",
            "10 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 43838.0703125\n",
            "10 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 261.6744079589844\n",
            "10 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 24805.001953125\n",
            "10 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 579.3836669921875\n",
            "11 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.50\n",
            "error 43918.97265625\n",
            "11 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 11841.72265625\n",
            "11 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 46752.6796875\n",
            "11 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 433.8797607421875\n",
            "11 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 27342.78125\n",
            "11 fc2\n",
            "Pruning ...\n",
            "time 0.86\n",
            "error 611.8163452148438\n",
            "model.decoder.embed_tokens.weight tensor(3.8851e-07)\n",
            "model.decoder.embed_positions.weight tensor(0.0005)\n",
            "model.decoder.final_layer_norm.weight tensor(0.)\n",
            "model.decoder.final_layer_norm.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.k_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.k_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.v_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.v_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.q_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.q_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.out_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.out_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn_layer_norm.weight tensor(0.)\n",
            "model.decoder.layers.0.self_attn_layer_norm.bias tensor(0.)\n",
            "model.decoder.layers.0.fc1.weight tensor(0.5000)\n",
            "model.decoder.layers.0.fc1.bias tensor(0.)\n",
            "model.decoder.layers.0.fc2.weight tensor(0.5000)\n",
            "62.254939794540405\n",
            "wikitext2\n",
            "Evaluating ...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "Perplexity: 36.996601\n",
            "ptb\n",
            "Evaluating ...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "Perplexity: 55.407249\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "c4\n",
            "Evaluating ...\n",
            "0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sparsegpt/opt.py\", line 337, in <module>\n",
            "    opt_eval(model, testloader, DEV, dataset, args.log_wandb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/sparsegpt/opt.py\", line 197, in opt_eval\n",
            "    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 329, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 230, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1845, in softmax\n",
            "    ret = input.softmax(dim, dtype=dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 14.75 GiB total capacity; 1.63 GiB already allocated; 8.81 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "!python opt.py facebook/opt-125m c4 --sparsity 0.5 --save sparse_opt/opt-125m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mrOL92aO5xy"
      },
      "source": [
        "Code above prints perplexity on `wikitext2`, `ptb` and `c4` benchmarks in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD9Zkgb-O21A"
      },
      "source": [
        "### Compare generations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSJIGizLkPm8"
      },
      "source": [
        "Let us compare generations produced by the dense and sparse model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-GzBUGsXic0o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, OPTForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ub-69himlTpZ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mQJtRPbekmXu"
      },
      "outputs": [],
      "source": [
        "# load dense model\n",
        "model_dn = OPTForCausalLM.from_pretrained('facebook/opt-125m', torch_dtype='auto').to(device)\n",
        "# load sparse model\n",
        "model_sp = OPTForCausalLM.from_pretrained('sparse_opt/opt-125m', torch_dtype='auto').to(device)\n",
        "# init tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Bqskug9-mXtR"
      },
      "outputs": [],
      "source": [
        "input_text = \"It takes a great deal of bravery\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fS7YWAAhnatI"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w61F2J0QoPTi"
      },
      "source": [
        "Completion by dense model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_xY5fSSnK2I",
        "outputId": "0f886d54-1cae-4337-e01c-ac551b4a43e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_ids = model_dn.generate(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmGPG1tnoci",
        "outputId": "0b86cc8d-8285-49ce-f0f1-67124598501b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It takes a great deal of bravery to get a job that pays you $15 an hour.\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output_ids[0].cpu(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_ids = model_sp.generate(input_ids)"
      ],
      "metadata": {
        "id": "4BBfNEL0LwWn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output_ids[0].cpu(), skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "xqRs6d5OLxDK",
        "outputId": "29b43185-eeec-44b1-e854-2429236afd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It takes a great deal of bravery to get a man to do something.\n",
            "\n",
            "I'm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "FJ23go8OIfV6",
        "outputId": "6f4b38d8-f5dd-493f-be16-01a6ef3d2c06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.28.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft) (0.22.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->peft) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LlamaTokenizer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "def get_tokenizer(model):\n",
        "    if \"llama\" in model.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model, use_fast=False)\n",
        "        # fix for transformer 4.28.0.dev0 compatibility\n",
        "        if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
        "            try:\n",
        "                tokenizer.bos_token_id = 1\n",
        "                tokenizer.eos_token_id = 2\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    return tokenizer\n",
        "\n",
        "def get_wikitext2(nsamples, seed, seqlen, model, tokenizer):\n",
        "\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_ptb(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
        "    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n",
        "    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_c4(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train'\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation'\n",
        "    )\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, model, tokenizer)\n",
        "    if 'ptb' in name:\n",
        "        return get_ptb(nsamples, seed, seqlen, model, tokenizer)\n",
        "    if 'c4' in name:\n",
        "        return get_c4(nsamples, seed, seqlen, model, tokenizer)"
      ],
      "metadata": {
        "id": "49-DgQuLNX4M"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "@torch.no_grad()\n",
        "def opt_eval(model, testenc, dev, dataset: str, log_wandb: bool = False):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // model.seqlen\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.decoder.layers\n",
        "\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask']\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        layer = layers[i].to(dev)\n",
        "\n",
        "        # if args.gmp:\n",
        "        #     subset = find_layers(layer)\n",
        "        #     for name in subset:\n",
        "        #         W = subset[name].weight.data\n",
        "        #         thresh = torch.sort(torch.abs(W.flatten()))[0][int(W.numel() * args.sparsity)]\n",
        "        #         W.data[torch.abs(W.data) <= thresh] = 0\n",
        "\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.decoder.final_layer_norm is not None:\n",
        "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)\n",
        "    if model.model.decoder.project_out is not None:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.decoder.final_layer_norm is not None:\n",
        "            hidden_states = model.model.decoder.final_layer_norm(hidden_states)\n",
        "        if model.model.decoder.project_out is not None:\n",
        "            hidden_states = model.model.decoder.project_out(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[\n",
        "            :, (i * model.seqlen):((i + 1) * model.seqlen)\n",
        "        ][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * model.seqlen\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
        "    print(f\"Perplexity: {ppl.item():3f}\")\n",
        "    if log_wandb:\n",
        "         wandb.log({f'{dataset}/perplexity': ppl.item()})\n",
        "\n",
        "    model.config.use_cache = use_cache"
      ],
      "metadata": {
        "id": "FgdHlOL8OoDL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model_sp)"
      ],
      "metadata": {
        "id": "dda5zPkUIfm7"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "4_V8OZpAIfpV"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "5IC34bup8-0q",
        "outputId": "1f35d538-b7cf-4b9b-dbab-3aa459d00b6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 0 || all params: 125239296 || trainable%: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "s_7PpC348-3N",
        "outputId": "fa33c788-3b23-4ce9-e721-4d5b40e508e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 589824 || all params: 125829120 || trainable%: 0.46875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n"
      ],
      "metadata": {
        "id": "7HnaDwDU8-5q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "SEe1l5Ak8-8Q",
        "outputId": "812a10a2-1895-49d5-f127-942aa48c3568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 01:37, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.518700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.817200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.385100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.325900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.224600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.213200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>3.981800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>3.478000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.436400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>3.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>3.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>3.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.308200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>3.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>3.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>3.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>3.229600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.193300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>3.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>3.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.102100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.221600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>3.375300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.388400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>3.209900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.635600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>3.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>3.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>3.366900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>3.230500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>3.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>3.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>3.213900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.943000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>3.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>3.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>3.394900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>3.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>3.067200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>3.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>3.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>3.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>3.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>3.143300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>3.164400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>3.238800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>3.190800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>3.327400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>3.593700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>3.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>3.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>3.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>3.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>3.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>3.323100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>3.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>3.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>3.048300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>3.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.467500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>3.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>3.429300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>3.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>3.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>3.044500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>3.089200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.918600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>3.113200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>3.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>3.152000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>3.260900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>3.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>3.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>3.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>3.033700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>3.414200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>3.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>2.829100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.589300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>3.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>3.248100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>3.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>3.186400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>3.170400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>3.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>3.199500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>3.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>3.209100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>2.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>3.076200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>3.402200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>3.127400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>3.183800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>3.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>3.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>3.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>3.473000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>3.220900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>3.321200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>3.495700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>3.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>3.061200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>3.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>3.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>3.053400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>2.949200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>3.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>3.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>3.154000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>3.068000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.840700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>3.090600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>3.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>3.367600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>3.026200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.384900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>3.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>3.250500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>3.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>3.425600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>3.137300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>3.163000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>3.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>3.246600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>3.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>3.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>3.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>3.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>2.959600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>3.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>3.435300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>3.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>3.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>3.105600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>3.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>3.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>3.092500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>3.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>3.158100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>2.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>2.967600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>3.244600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>3.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.110700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>3.128000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>3.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>3.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>3.203700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>3.404300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>3.126200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>3.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>3.067400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>3.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>3.264200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>3.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>3.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>3.026400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>3.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>3.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>3.154200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>3.142200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>3.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.056900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=3.262033259868622, metrics={'train_runtime': 98.0933, 'train_samples_per_second': 32.622, 'train_steps_per_second': 2.039, 'total_flos': 152460529385472.0, 'train_loss': 3.262033259868622, 'epoch': 1.28})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('opt-lora')"
      ],
      "metadata": {
        "id": "T1CraOJs8--2"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "ikcA6FG-WH1d",
        "outputId": "852537f0-bd40-41b5-da22-b885e14b49fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bloom.py      LICENSE        \u001b[0m\u001b[01;34mopt-lora\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  sparsegpt.py\n",
            "datautils.py  llama.py       opt.py     quant.py      \u001b[01;34msparse_opt\u001b[0m/\n",
            "demo.ipynb    modelutils.py  \u001b[01;34moutputs\u001b[0m/   README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ky5U9elZn-pL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"./opt-lora\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = OPTForCausalLM.from_pretrained('facebook/opt-125m', torch_dtype='auto').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = tokenizer(\"Two things are infinite: \", return_tensors='pt').to(device)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "TROdfrvoVpg1",
        "outputId": "45dd773b-801a-429d-92fa-123ac3846218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Two things are infinite:  1. The universe is infinite.  2. The universe is infinite.                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYFJRahuWbve"
      },
      "execution_count": 55,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}