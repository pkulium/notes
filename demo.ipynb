{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pkulium/notes/blob/main/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbRrWtX0PXVr"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/IST-DASLab/sparsegpt/blob/master/demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMUp4UrWjp-8"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VdbD9blm6j_r"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhSblKg_jter"
      },
      "source": [
        "Clone repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nCz469NhV3c",
        "outputId": "5af90157-5a0c-4c0c-b013-e18e737e0ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'sparsegpt' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IST-DASLab/sparsegpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbM_bJODjyBg"
      },
      "source": [
        "### Pruning example\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om0QSLnLj8JN"
      },
      "source": [
        "Below we will show an example of SparseGPT applied to OPT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9NTGmD4iVK7",
        "outputId": "63888067-273f-4a84-857d-47914da42f9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/sparsegpt\n"
          ]
        }
      ],
      "source": [
        "%cd sparsegpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbiDyjx9j61I"
      },
      "source": [
        "Crerate directory to store prune model(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pJ-jauI-iyvi"
      },
      "outputs": [],
      "source": [
        "!mkdir -p sparse_opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGLiExo5Ksc4"
      },
      "source": [
        "We will use `opt.py` script to prune the model.\n",
        "Select one of the following OPT versions to fit into colab (with `bitsandbytes` one should be able to use larger 6.7b and 13b models):\n",
        "* facebook/opt-125m\n",
        "* facebook/opt-350m\n",
        "* facebook/opt-1.3b\n",
        "\n",
        "To prune the model select dataset for calibration (`c4`, `ptb` or `wikitext`). The SparseGPT paper uses `c4` by default.\n",
        "\n",
        "One can prune model to uniform sparsity with SparseGPT either with unstructured pruning or semistructured `N:M` pattern.\n",
        "\n",
        "To apply unstructured pruning specify `--sparsity` - floating point number in `[0, 1]`.\n",
        "\n",
        "For semitstructured specify `--prunen` and `--prunem` arguments - integer numbers.\n",
        "\n",
        "To apply magnitude pruning instead of SparseGPT select `--gmp` option.\n",
        "\n",
        "To apply quantization on top of sparsity specify `--wbits`.\n",
        "\n",
        "In the example below we prune `acebook/opt-125m` to 0.5 unstructured sparsity via SparseGPT. Try different options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxucjXmCibnI",
        "outputId": "0e1c9bf3-271e-4aeb-c02e-db3494bba5fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-25 20:25:15.324885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Starting ...\n",
            "Ready.\n",
            "0 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.54\n",
            "error 13941.587890625\n",
            "0 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 547.517578125\n",
            "0 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 14149.1669921875\n",
            "0 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 6.390275001525879\n",
            "0 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 2260.54931640625\n",
            "0 fc2\n",
            "Pruning ...\n",
            "time 0.71\n",
            "error 36.479331970214844\n",
            "1 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.60\n",
            "error 9682.548828125\n",
            "1 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.28\n",
            "error 641.5951538085938\n",
            "1 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.28\n",
            "error 4022.392578125\n",
            "1 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.24\n",
            "error 4.027432441711426\n",
            "1 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 6266.3798828125\n",
            "1 fc2\n",
            "Pruning ...\n",
            "time 0.69\n",
            "error 14.219343185424805\n",
            "2 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.48\n",
            "error 15789.4208984375\n",
            "2 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.20\n",
            "error 1678.421875\n",
            "2 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 14260.4677734375\n",
            "2 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 9.766499519348145\n",
            "2 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 5069.49609375\n",
            "2 fc2\n",
            "Pruning ...\n",
            "time 0.71\n",
            "error 10.688285827636719\n",
            "3 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.48\n",
            "error 14277.4296875\n",
            "3 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 2320.9306640625\n",
            "3 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 14309.693359375\n",
            "3 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 15.976364135742188\n",
            "3 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 3086.05517578125\n",
            "3 fc2\n",
            "Pruning ...\n",
            "time 0.98\n",
            "error 0.48200276494026184\n",
            "4 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 27314.748046875\n",
            "4 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 3171.47998046875\n",
            "4 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 29181.6953125\n",
            "4 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 24.271169662475586\n",
            "4 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 7956.150390625\n",
            "4 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 29.87113380432129\n",
            "5 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 29646.88671875\n",
            "5 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 2876.80419921875\n",
            "5 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 34879.80078125\n",
            "5 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 38.6750602722168\n",
            "5 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 8126.98583984375\n",
            "5 fc2\n",
            "Pruning ...\n",
            "time 0.69\n",
            "error 71.48601531982422\n",
            "6 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.57\n",
            "error 31993.59375\n",
            "6 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.24\n",
            "error 3910.056884765625\n",
            "6 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.23\n",
            "error 33820.984375\n",
            "6 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.26\n",
            "error 48.310401916503906\n",
            "6 fc1\n",
            "Pruning ...\n",
            "time 0.23\n",
            "error 8047.31103515625\n",
            "6 fc2\n",
            "Pruning ...\n",
            "time 1.04\n",
            "error 101.61123657226562\n",
            "7 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.50\n",
            "error 39261.01171875\n",
            "7 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 4540.8515625\n",
            "7 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 39027.109375\n",
            "7 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 82.00370788574219\n",
            "7 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 10329.146484375\n",
            "7 fc2\n",
            "Pruning ...\n",
            "time 0.72\n",
            "error 137.32748413085938\n",
            "8 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.49\n",
            "error 40062.2421875\n",
            "8 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 6433.7001953125\n",
            "8 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.20\n",
            "error 44656.8046875\n",
            "8 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.16\n",
            "error 160.28634643554688\n",
            "8 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 14348.458984375\n",
            "8 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 227.66287231445312\n",
            "9 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.58\n",
            "error 47157.65625\n",
            "9 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.30\n",
            "error 7610.232421875\n",
            "9 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.31\n",
            "error 51200.7421875\n",
            "9 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.29\n",
            "error 271.1785583496094\n",
            "9 fc1\n",
            "Pruning ...\n",
            "time 0.29\n",
            "error 19706.921875\n",
            "9 fc2\n",
            "Pruning ...\n",
            "time 0.87\n",
            "error 372.0923156738281\n",
            "10 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.51\n",
            "error 44735.5078125\n",
            "10 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 9526.2900390625\n",
            "10 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 43838.0703125\n",
            "10 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 261.6744079589844\n",
            "10 fc1\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 24805.001953125\n",
            "10 fc2\n",
            "Pruning ...\n",
            "time 0.70\n",
            "error 579.3836669921875\n",
            "11 self_attn.k_proj\n",
            "Pruning ...\n",
            "time 0.50\n",
            "error 43918.97265625\n",
            "11 self_attn.v_proj\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 11841.72265625\n",
            "11 self_attn.q_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 46752.6796875\n",
            "11 self_attn.out_proj\n",
            "Pruning ...\n",
            "time 0.17\n",
            "error 433.8797607421875\n",
            "11 fc1\n",
            "Pruning ...\n",
            "time 0.18\n",
            "error 27342.78125\n",
            "11 fc2\n",
            "Pruning ...\n",
            "time 0.86\n",
            "error 611.8163452148438\n",
            "model.decoder.embed_tokens.weight tensor(3.8851e-07)\n",
            "model.decoder.embed_positions.weight tensor(0.0005)\n",
            "model.decoder.final_layer_norm.weight tensor(0.)\n",
            "model.decoder.final_layer_norm.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.k_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.k_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.v_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.v_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.q_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.q_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn.out_proj.weight tensor(0.5000)\n",
            "model.decoder.layers.0.self_attn.out_proj.bias tensor(0.)\n",
            "model.decoder.layers.0.self_attn_layer_norm.weight tensor(0.)\n",
            "model.decoder.layers.0.self_attn_layer_norm.bias tensor(0.)\n",
            "model.decoder.layers.0.fc1.weight tensor(0.5000)\n",
            "model.decoder.layers.0.fc1.bias tensor(0.)\n",
            "model.decoder.layers.0.fc2.weight tensor(0.5000)\n",
            "62.254939794540405\n",
            "wikitext2\n",
            "Evaluating ...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "Perplexity: 36.996601\n",
            "ptb\n",
            "Evaluating ...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "Perplexity: 55.407249\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "c4\n",
            "Evaluating ...\n",
            "0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sparsegpt/opt.py\", line 337, in <module>\n",
            "    opt_eval(model, testloader, DEV, dataset, args.log_wandb)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/sparsegpt/opt.py\", line 197, in opt_eval\n",
            "    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 329, in forward\n",
            "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/opt/modeling_opt.py\", line 230, in forward\n",
            "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 1845, in softmax\n",
            "    ret = input.softmax(dim, dtype=dtype)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 14.75 GiB total capacity; 1.63 GiB already allocated; 8.81 MiB free; 1.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
          ]
        }
      ],
      "source": [
        "!python opt.py facebook/opt-125m c4 --sparsity 0.5 --save sparse_opt/opt-125m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mrOL92aO5xy"
      },
      "source": [
        "Code above prints perplexity on `wikitext2`, `ptb` and `c4` benchmarks in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD9Zkgb-O21A"
      },
      "source": [
        "### Compare generations\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSJIGizLkPm8"
      },
      "source": [
        "Let us compare generations produced by the dense and sparse model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "-GzBUGsXic0o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, OPTForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ub-69himlTpZ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "mQJtRPbekmXu"
      },
      "outputs": [],
      "source": [
        "# load dense model\n",
        "model_dn = OPTForCausalLM.from_pretrained('facebook/opt-125m', torch_dtype='auto').to(device)\n",
        "# load sparse model\n",
        "model_sp = OPTForCausalLM.from_pretrained('sparse_opt/opt-125m', torch_dtype='auto').to(device)\n",
        "# init tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Bqskug9-mXtR"
      },
      "outputs": [],
      "source": [
        "input_text = \"It takes a great deal of bravery\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fS7YWAAhnatI"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w61F2J0QoPTi"
      },
      "source": [
        "Completion by dense model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_xY5fSSnK2I",
        "outputId": "0f886d54-1cae-4337-e01c-ac551b4a43e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "output_ids = model_dn.generate(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRmGPG1tnoci",
        "outputId": "0b86cc8d-8285-49ce-f0f1-67124598501b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It takes a great deal of bravery to get a job that pays you $15 an hour.\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output_ids[0].cpu(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "4BBfNEL0LwWn"
      },
      "outputs": [],
      "source": [
        "output_ids = model_sp.generate(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqRs6d5OLxDK",
        "outputId": "29b43185-eeec-44b1-e854-2429236afd39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It takes a great deal of bravery to get a man to do something.\n",
            "\n",
            "I'm\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output_ids[0].cpu(), skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ23go8OIfV6",
        "outputId": "6f4b38d8-f5dd-493f-be16-01a6ef3d2c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.28.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft) (0.22.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.16.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->peft) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "49-DgQuLNX4M"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LlamaTokenizer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "def get_tokenizer(model):\n",
        "    if \"llama\" in model.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model, use_fast=False)\n",
        "        # fix for transformer 4.28.0.dev0 compatibility\n",
        "        if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
        "            try:\n",
        "                tokenizer.bos_token_id = 1\n",
        "                tokenizer.eos_token_id = 2\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "        # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    return tokenizer\n",
        "\n",
        "def get_wikitext2(nsamples, seed, seqlen, model, tokenizer):\n",
        "\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_ptb(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
        "    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n",
        "    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_c4(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train'\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation'\n",
        "    )\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n",
        "    tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, model, tokenizer)\n",
        "    if 'ptb' in name:\n",
        "        return get_ptb(nsamples, seed, seqlen, model, tokenizer)\n",
        "    if 'c4' in name:\n",
        "        return get_c4(nsamples, seed, seqlen, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "FgdHlOL8OoDL"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "@torch.no_grad()\n",
        "def opt_eval(model, testenc, dev, dataset: str, log_wandb: bool = False):\n",
        "    print('Evaluating ...')\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // model.seqlen\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.decoder.layers\n",
        "\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.to(dev)\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {'i': 0, 'attention_mask': None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache['i']] = inp\n",
        "            cache['i'] += 1\n",
        "            cache['attention_mask'] = kwargs['attention_mask']\n",
        "            raise ValueError\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.decoder.embed_tokens = model.model.decoder.embed_tokens.cpu()\n",
        "    model.model.decoder.embed_positions = model.model.decoder.embed_positions.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_out') and model.model.decoder.project_out:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.cpu()\n",
        "    if hasattr(model.model.decoder, 'project_in') and model.model.decoder.project_in:\n",
        "        model.model.decoder.project_in = model.model.decoder.project_in.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache['attention_mask']\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        layer = layers[i].to(dev)\n",
        "\n",
        "        # if args.gmp:\n",
        "        #     subset = find_layers(layer)\n",
        "        #     for name in subset:\n",
        "        #         W = subset[name].weight.data\n",
        "        #         thresh = torch.sort(torch.abs(W.flatten()))[0][int(W.numel() * args.sparsity)]\n",
        "        #         W.data[torch.abs(W.data) <= thresh] = 0\n",
        "\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.decoder.final_layer_norm is not None:\n",
        "        model.model.decoder.final_layer_norm = model.model.decoder.final_layer_norm.to(dev)\n",
        "    if model.model.decoder.project_out is not None:\n",
        "        model.model.decoder.project_out = model.model.decoder.project_out.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.decoder.final_layer_norm is not None:\n",
        "            hidden_states = model.model.decoder.final_layer_norm(hidden_states)\n",
        "        if model.model.decoder.project_out is not None:\n",
        "            hidden_states = model.model.decoder.project_out(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[\n",
        "            :, (i * model.seqlen):((i + 1) * model.seqlen)\n",
        "        ][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "        neg_log_likelihood = loss.float() * model.seqlen\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
        "    print(f\"Perplexity: {ppl.item():3f}\")\n",
        "    if log_wandb:\n",
        "         wandb.log({f'{dataset}/perplexity': ppl.item()})\n",
        "\n",
        "    model.config.use_cache = use_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "dda5zPkUIfm7"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model_sp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "4_V8OZpAIfpV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IC34bup8-0q",
        "outputId": "1f35d538-b7cf-4b9b-dbab-3aa459d00b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 0 || all params: 125239296 || trainable%: 0.0\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_7PpC348-3N",
        "outputId": "fa33c788-3b23-4ce9-e721-4d5b40e508e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 589824 || all params: 125829120 || trainable%: 0.46875\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "7HnaDwDU8-5q"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "data = load_dataset(\"Abirate/english_quotes\")\n",
        "data = data.map(lambda samples: tokenizer(samples['quote']), batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SEe1l5Ak8-8Q",
        "outputId": "812a10a2-1895-49d5-f127-942aa48c3568"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 01:37, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.425000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.518700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.817200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.385100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.325900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.224600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.213200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>3.981800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>3.478000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.436400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>3.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>3.281400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>3.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.308200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>3.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>3.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>3.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>3.229600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.289300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.193300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>3.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>3.274100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.114000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.102100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.221600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>3.375300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.388400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>3.209900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.635600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>3.732100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>3.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>3.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>3.366900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>3.230500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>3.208700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>3.142500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>3.213900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.943000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>3.243300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>3.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>3.394900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>3.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>3.067200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>3.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>3.234100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>3.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>3.353800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.106400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>3.143300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>3.164400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.941400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>3.238800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>3.190800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>3.327400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>3.593700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>3.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.262200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>3.075500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>3.130200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>3.641200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>3.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>3.323100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>3.109700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>3.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>3.048300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>3.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.467500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>3.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>3.429300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>3.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>3.125200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>3.044500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>3.089200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.918600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>3.113200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>3.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>3.152000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>3.260900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>3.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>3.015800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>3.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>3.033700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>3.414200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>3.110000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>2.829100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.589300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>3.152400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>3.248100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>3.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>3.186400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>3.170400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>3.327100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>3.199500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>3.104400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>3.209100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>2.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>3.076200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>3.402200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>3.127400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>3.183800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>3.061800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>3.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>3.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>3.473000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>3.220900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>3.321200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>3.495700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>3.181400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>3.061200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>3.159400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>3.020200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>3.053400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>2.949200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>3.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>3.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>3.154000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>3.068000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.840700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>3.090600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>3.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>3.367600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>3.026200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.384900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>3.045000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>3.250500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>3.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>3.425600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>3.137300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>3.163000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>3.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>3.246600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>3.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>3.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>3.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>3.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>2.959600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>3.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>3.435300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>3.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>3.267100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>3.105600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.010300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>3.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>3.149700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>3.092500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>3.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>3.158100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>2.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>2.967600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>3.244600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>3.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.110700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>3.128000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>3.014400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>3.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>3.203700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>3.404300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>3.126200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>3.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>3.067400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>3.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>3.264200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>3.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>3.132500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>3.026400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>3.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>3.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>3.154200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>3.142200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>3.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.056900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=200, training_loss=3.262033259868622, metrics={'train_runtime': 98.0933, 'train_samples_per_second': 32.622, 'train_steps_per_second': 2.039, 'total_flos': 152460529385472.0, 'train_loss': 3.262033259868622, 'epoch': 1.28})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100,\n",
        "        max_steps=200,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "T1CraOJs8--2"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained('opt-lora')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikcA6FG-WH1d",
        "outputId": "852537f0-bd40-41b5-da22-b885e14b49fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bloom.py      LICENSE        \u001b[0m\u001b[01;34mopt-lora\u001b[0m/  \u001b[01;34m__pycache__\u001b[0m/  sparsegpt.py\n",
            "datautils.py  llama.py       opt.py     quant.py      \u001b[01;34msparse_opt\u001b[0m/\n",
            "demo.ipynb    modelutils.py  \u001b[01;34moutputs\u001b[0m/   README.md\n"
          ]
        }
      ],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ky5U9elZn-pL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"./opt-lora\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = OPTForCausalLM.from_pretrained('facebook/opt-125m', torch_dtype='auto').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('facebook/opt-125m')\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TROdfrvoVpg1",
        "outputId": "45dd773b-801a-429d-92fa-123ac3846218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Two things are infinite:  1. The universe is infinite.  2. The universe is infinite.                                   \n"
          ]
        }
      ],
      "source": [
        "batch = tokenizer(\"Two things are infinite: \", return_tensors='pt').to(device)\n",
        "\n",
        "with torch.cuda.amp.autocast():\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50)\n",
        "\n",
        "print('\\n\\n', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hYFJRahuWbve"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data\n",
        "m_values = [\"5%\", \"10%\", \"20%\", \"33%\", \"50%\"]\n",
        "\n",
        "# MNIST data for alpha = 1\n",
        "FG_MNIST = [81.86, 81.77, 74.58, 77.24, 75.66]\n",
        "CONTRA_MNIST = [84.21, 83.59, 82.18, 80.16, 81.64]\n",
        "Ours_MNIST = [91.92, 91.38, 91.29, 89.9, 89.35]\n",
        "\n",
        "# CIFAR-10 data for alpha = 1\n",
        "FG_CIFAR10 = [73.63, 74.84, 72.21, 70.08, 69.11]\n",
        "CONTRA_CIFAR10 = [76.88, 77.95, 74.66, 72.97, 73.70]\n",
        "Ours_CIFAR10 = [81.23, 81.5, 81.43, 81.44, 79.85]\n",
        "\n",
        "# Create subplots\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Plot MNIST data\n",
        "axs[0].plot(m_values, FG_MNIST, marker='o', label='FG')\n",
        "axs[0].plot(m_values, CONTRA_MNIST, marker='o', label='CONTRA')\n",
        "axs[0].plot(m_values, Ours_MNIST, marker='o', label='Ours')\n",
        "axs[0].set_title('MNIST')\n",
        "axs[0].set_xlabel('Malicious Fraction (m)')\n",
        "axs[0].set_ylabel('Accuracy (%)')\n",
        "axs[0].legend()\n",
        "axs[0].grid(True)\n",
        "\n",
        "# Plot CIFAR-10 data\n",
        "axs[1].plot(m_values, FG_CIFAR10, marker='o', label='FG')\n",
        "axs[1].plot(m_values, CONTRA_CIFAR10, marker='o', label='CONTRA')\n",
        "axs[1].plot(m_values, Ours_CIFAR10, marker='o', label='Ours')\n",
        "axs[1].set_title('CIFAR-10')\n",
        "axs[1].set_xlabel('Malicious Fraction (m)')\n",
        "axs[1].set_ylabel('Accuracy (%)')\n",
        "axs[1].legend()\n",
        "axs[1].grid(True)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure in IEEE conference standard format (usually a high-resolution format like PNG or PDF)\n",
        "plt.savefig(\"results_plot.pdf\", dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
